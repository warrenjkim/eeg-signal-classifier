{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00685528-1d7f-4b43-a9fa-03c5512ac5e4",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63c692-5f04-4064-9327-2ca6659377ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torchinfo\n",
    "import optuna\n",
    "\n",
    "import nndl.models.CNN as cnn\n",
    "import nndl.models.CNNLSTM as cnnlstm\n",
    "import nndl.utils as utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291375a-0757-46bd-912c-214c03d2f302",
   "metadata": {},
   "source": [
    "# **Load data from localhost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c22e93-a467-4574-864d-205dd29910a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"./project_data/X_test.npy\")\n",
    "y_test = np.load(\"./project_data/y_test.npy\") - 769\n",
    "person_train_valid = np.load(\"./project_data/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"./project_data/X_train_valid.npy\")\n",
    "print(X_train_valid.shape)\n",
    "y_train_valid = np.load(\"./project_data/y_train_valid.npy\") - 769\n",
    "person_test = np.load(\"./project_data/person_test.npy\")\n",
    "\n",
    "print(X_train_valid.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc5ab2-0755-451d-b091-4a85f2053a53",
   "metadata": {},
   "source": [
    "# **Data Prep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123553f6-97ae-4903-b473-5fa82a794113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(X,y,sub_sample,average,noise,channel_dropout,time_reverse):\n",
    "\n",
    "    total_X = None\n",
    "    total_y = None\n",
    "    X = X[:,:,0:800]\n",
    "    print('Shape of X after trimming:',X.shape)\n",
    "\n",
    "    X_max, _ = torch.max(X.view(X.size(0), X.size(1), -1, sub_sample), axis=3)\n",
    "\n",
    "\n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    print('Shape of X after maxpooling:',total_X.shape)\n",
    "\n",
    "    X_average = torch.mean(X.view(X.size(0), X.size(1), -1, average), axis=3)\n",
    "    X_average = X_average + torch.normal(0.0, 0.5, X_average.shape)\n",
    "\n",
    "    total_X = torch.cat((total_X, X_average), dim=0)\n",
    "    total_y = torch.cat((total_y, y))\n",
    "    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
    "\n",
    "    for i in range(sub_sample):\n",
    "\n",
    "        X_subsample = X[:, :, i::sub_sample] + \\\n",
    "                            (torch.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "\n",
    "        total_X = torch.cat((total_X, X_subsample), dim=0)\n",
    "        print(total_y.view(-1,1).shape)\n",
    "        print(y.view(-1,1).shape)\n",
    "        total_y = torch.cat((total_y, y))\n",
    "\n",
    "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
    "    print('Shape of Y:',total_y.shape)\n",
    "\n",
    "    if channel_dropout != 0:\n",
    "        mask = (torch.rand(total_X.shape[0], total_X.shape[1]) >= channel_dropout).unsqueeze(2)\n",
    "        X_dropout = mask * total_X\n",
    "        total_X = torch.cat((total_X, X_dropout))\n",
    "        total_y = torch.cat((total_y, total_y))\n",
    "\n",
    "        print(f'Shape of X after channel dropout {total_X.shape}')\n",
    "        print(f'Shape of Y: {total_y.shape}')\n",
    "\n",
    "    if time_reverse != 0:\n",
    "        mask = (torch.rand(total_X.shape[0]) >= time_reverse)\n",
    "        X_reverse = torch.flip((total_X[mask]), [2])\n",
    "        total_X = torch.cat((total_X, X_reverse))\n",
    "        total_y = torch.cat((total_y, total_y[mask]))\n",
    "\n",
    "        print(f'Shape of X after reverse {total_X.shape}')\n",
    "        print(f'Shape of Y: {total_y.shape}')\n",
    "\n",
    "\n",
    "    return total_X,total_y\n",
    "    return total_X,total_y\n",
    "\n",
    "\n",
    "def test_data_prep(X):\n",
    "\n",
    "    total_X = None\n",
    "\n",
    "    X = X[:,:,0:800]\n",
    "    print('Shape of X after trimming:', X.shape)\n",
    "    X_max, _ = torch.max(X.view(X.size(0), X.size(1), -1, 2), axis=3)\n",
    "\n",
    "    total_X = X_max\n",
    "    print('Shape of X after maxpooling:',total_X.shape)\n",
    "\n",
    "    return total_X\n",
    "\n",
    "input_size = 22\n",
    "hidden_size = 256\n",
    "num_layers = 10\n",
    "num_classes = 4\n",
    "learning_rate = 1e-4\n",
    "batch_size = 200\n",
    "num_epochs = 100\n",
    "dropout = 0.3\n",
    "\n",
    "folds = 10\n",
    "split_seed = 1\n",
    "subsample = 2\n",
    "average = 2\n",
    "channel_dropout = 0.2\n",
    "time_reverse = 0.2\n",
    "noise = True\n",
    "\n",
    "'''\n",
    "This was the old data prep.\n",
    "The code below is just split up.\n",
    "Split data, then split up X_train, X_val, X_test by subject and create dataloaders\n",
    "Then dataprep and dataloader is performed for x_train etc\n",
    "kept in case of error then can go back\n",
    "i could have put them in lists\n",
    "'''\n",
    "'''\n",
    "X_train_valid = torch.Tensor(X_train_valid)\n",
    "y_train_valid = torch.Tensor(y_train_valid)\n",
    "\n",
    "print(f'SOME {X_train_valid.shape}')\n",
    "indices = torch.randperm(X_train_valid.shape[0])\n",
    "split_idx = int(X_train_valid.shape[0] * ((folds-1)/folds))\n",
    "X_train, X_valid = X_train_valid[indices[:split_idx]], X_train_valid[indices[split_idx:]]\n",
    "y_train, y_valid = y_train_valid[indices[:split_idx]], y_train_valid[indices[split_idx:]]\n",
    "X_test, y_test = torch.Tensor(X_test), torch.Tensor(y_test)\n",
    "\n",
    "print('Prepping Training Data')\n",
    "X_train, y_train = data_prep(X_train, y_train, subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nPrepping Validation Data')\n",
    "X_valid, y_valid = data_prep(X_valid, y_valid, subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nPrepping Test Data')\n",
    "X_test = test_data_prep(X_test)\n",
    "print('\\nFINISHED PREP\\n')\n",
    "\n",
    "print('Final shape of training set:', X_train.shape)\n",
    "print('Final shape of validation set:', X_valid.shape)\n",
    "print('Final shape of test set:', X_test.shape)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'TRAIN_DATA {len(train_loader)}\\nVALID_DATA {len(val_loader)}\\nTEST_DATA {len(test_loader)}')\n",
    "'''\n",
    "\n",
    "X_train_valid = torch.Tensor(X_train_valid)\n",
    "y_train_valid = torch.Tensor(y_train_valid)\n",
    "\n",
    "print(f'SOME {X_train_valid.shape}')\n",
    "indices = torch.randperm(X_train_valid.shape[0])\n",
    "#print(indices)\n",
    "split_idx = int(X_train_valid.shape[0] * ((folds-1)/folds))\n",
    "#print(split_idx)\n",
    "X_train, X_valid = X_train_valid[indices[:split_idx]], X_train_valid[indices[split_idx:]]\n",
    "y_train, y_valid = y_train_valid[indices[:split_idx]], y_train_valid[indices[split_idx:]]\n",
    "X_test, y_test = torch.Tensor(X_test), torch.Tensor(y_test)\n",
    "\n",
    "person_train_valid, person_test = torch.Tensor(person_train_valid), torch.Tensor(person_test)\n",
    "person_train, person_valid = person_train_valid[indices[:split_idx]], person_train_valid[indices[split_idx:]]\n",
    "\n",
    "#check it matches\n",
    "'''\n",
    "index_zero = (indices == 0).nonzero(as_tuple=False) #index in indices that hold index 0 in X_train_valid and person_train_valid\n",
    "print(index_zero)\n",
    "print(indices[index_zero])\n",
    "if index_zero > 1902: #in validation\n",
    "  print(X_train_valid[0])\n",
    "  print(X_train[index_zero - 1903])\n",
    "  print(person_train[index_zero - 1903])\n",
    "  print(person_train_valid[0])\n",
    "else:\n",
    "  print(X_train_valid[0])\n",
    "  print(X_train[index_zero])\n",
    "  print(person_train[index_zero])\n",
    "  print(person_train_valid[0])\n",
    "'''\n",
    "\n",
    "################################################\n",
    "#seperate X_train by subject using person_train\n",
    "################################################\n",
    "\n",
    "s_train = [[], [], [], [], [], [], [], [], []]\n",
    "s_y_train = [[], [], [], [], [], [], [], [], []]\n",
    "count_train = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "for (i, j) in zip(person_train, range(len(person_train))):\n",
    "  #i is a np.darray from subject_train, i[0] gives us the subject\n",
    "  #j is the index of i in subject_train that corresponds to the index in subject_train\n",
    "  s_train[int(i[0])].append(X_train[j])\n",
    "  s_y_train[int(i[0])].append(y_train[j])\n",
    "\n",
    "  #for length check\n",
    "  count_train[int(i[0])]+=1\n",
    "\n",
    "print('subject counts: ', count_train)\n",
    "print('subject count sum: ', sum(count_train))\n",
    "\n",
    "#lebron\n",
    "for (i, j, k) in zip(count_train, s_train, s_y_train):\n",
    "  if i != len(j) or i != len(k):\n",
    "    print(\"length issue\")\n",
    "\n",
    "\n",
    "print('\\nsubject 0')\n",
    "s0_train, s0_y_train = data_prep(torch.Tensor(np.asarray(s_train[0])), torch.Tensor(np.asarray(s_y_train[0])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 1')\n",
    "s1_train, s1_y_train = data_prep(torch.Tensor(np.asarray(s_train[1])), torch.Tensor(np.asarray(s_y_train[1])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 2')\n",
    "s2_train, s2_y_train = data_prep(torch.Tensor(np.asarray(s_train[2])), torch.Tensor(np.asarray(s_y_train[2])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 3')\n",
    "s3_train, s3_y_train = data_prep(torch.Tensor(np.asarray(s_train[3])), torch.Tensor(np.asarray(s_y_train[3])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 4')\n",
    "s4_train, s4_y_train = data_prep(torch.Tensor(np.asarray(s_train[4])), torch.Tensor(np.asarray(s_y_train[4])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 5')\n",
    "s5_train, s5_y_train = data_prep(torch.Tensor(np.asarray(s_train[5])), torch.Tensor(np.asarray(s_y_train[5])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 6')\n",
    "s6_train, s6_y_train = data_prep(torch.Tensor(np.asarray(s_train[6])), torch.Tensor(np.asarray(s_y_train[6])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 7')\n",
    "s7_train, s7_y_train = data_prep(torch.Tensor(np.asarray(s_train[7])), torch.Tensor(np.asarray(s_y_train[7])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 8')\n",
    "s8_train, s8_y_train = data_prep(torch.Tensor(np.asarray(s_train[8])), torch.Tensor(np.asarray(s_y_train[8])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "\n",
    "#############################################\n",
    "#seperate X_val by subject using person_valid\n",
    "#############################################\n",
    "print(X_valid.shape)\n",
    "\n",
    "s_valid = [[], [], [], [], [], [], [], [], []]\n",
    "s_y_valid = [[], [], [], [], [], [], [], [], []]\n",
    "count_val = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "for (i, j) in zip(person_valid, range(len(person_valid))):\n",
    "  #i is a np.darray from subject_train, i[0] gives us the subject\n",
    "  #j is the index of i in subject_train that corresponds to the index in subject_train\n",
    "  s_valid[int(i[0])].append(X_valid[j])\n",
    "  s_y_valid[int(i[0])].append(y_valid[j])\n",
    "\n",
    "  #for length check\n",
    "  count_val[int(i[0])]+=1\n",
    "\n",
    "print('val counts: ', count_val)\n",
    "print('val count sum: ', sum(count_val))\n",
    "\n",
    "#lebron\n",
    "for (i, j, k) in zip(count_val, s_valid, s_y_valid):\n",
    "  if i != len(j) or i != len(k):\n",
    "    print(\"length issue\")\n",
    "\n",
    "print('\\nsubject 0')\n",
    "s0_valid, s0_y_valid = data_prep(torch.Tensor(np.asarray(s_valid[0])), torch.Tensor(np.asarray(s_y_valid[0])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 1')\n",
    "s1_valid, s1_y_valid = data_prep(torch.Tensor(np.asarray(s_valid[1])), torch.Tensor(np.asarray(s_y_valid[1])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 2')\n",
    "s2_valid, s2_y_valid = data_prep(torch.Tensor(np.asarray(s_valid[2])), torch.Tensor(np.asarray(s_y_valid[2])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 3')\n",
    "s3_valid, s3_y_valid = data_prep(torch.Tensor(np.asarray(s_valid[3])), torch.Tensor(np.asarray(s_y_valid[3])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 4')\n",
    "s4_valid, s4_y_valid = data_prep(torch.Tensor(np.asarray(s_valid[4])), torch.Tensor(np.asarray(s_y_valid[4])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 5')\n",
    "s5_valid, s5_y_valid = data_prep(torch.Tensor(np.asarray(s_valid[5])), torch.Tensor(np.asarray(s_y_valid[5])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 6')\n",
    "s6_valid, s6_y_valid = data_prep(torch.Tensor(np.asarray(s_valid[6])), torch.Tensor(np.asarray(s_y_valid[6])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 7')\n",
    "s7_valid, s7_y_valid = data_prep(torch.Tensor(np.asarray(s_valid[7])), torch.Tensor(np.asarray(s_y_valid[7])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nsubject 8')\n",
    "s8_valid, s8_y_valid = data_prep(torch.Tensor(np.asarray(s_valid[8])), torch.Tensor(np.asarray(s_y_valid[8])), subsample, average, noise, channel_dropout, time_reverse)\n",
    "\n",
    "#############################################\n",
    "#seperate X_test by subject using person_test\n",
    "#############################################\n",
    "print(X_test.shape)\n",
    "\n",
    "s_test = [[], [], [], [], [], [], [], [], []]\n",
    "s_y_test = [[], [], [], [], [], [], [], [], []]\n",
    "count_test = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "for (i, j) in zip(person_test, range(len(person_test))):\n",
    "  #i is a np.darray from person_test, i[0] gives us the subject\n",
    "  #j is the index of i in person_test that corresponds to the index in X_test\n",
    "  s_test[int(i[0])].append(X_test[j])\n",
    "  s_y_test[int(i[0])].append(y_test[j])\n",
    "\n",
    "  #for length check\n",
    "  count_test[int(i[0])]+=1\n",
    "\n",
    "print('test counts: ', count_test)\n",
    "print('test count sum: ', sum(count_test))\n",
    "\n",
    "#lebron length check\n",
    "for (i, j, k) in zip(count_test, s_test, s_y_test):\n",
    "  if i != len(j) or i != len(k):\n",
    "    print(\"length issue\")\n",
    "\n",
    "#Look in my eyes, tell me your tale Do you see the road, the map to my soul? Look, tell me the signs whenever the smoke clear out of my face Am I picture-perfect or do I look fried?\n",
    "print('\\nsubject 0')\n",
    "s0_test, s0_y_test = test_data_prep(torch.Tensor(np.asarray(s_test[0]))), torch.Tensor(np.asarray(s_y_test[0]))\n",
    "print('\\nsubject 1')\n",
    "s1_test, s1_y_test = test_data_prep(torch.Tensor(np.asarray(s_test[1]))), torch.Tensor(np.asarray(s_y_test[1]))\n",
    "print('\\nsubject 2')\n",
    "s2_test, s2_y_test = test_data_prep(torch.Tensor(np.asarray(s_test[2]))), torch.Tensor(np.asarray(s_y_test[2]))\n",
    "print('\\nsubject 3')\n",
    "s3_test, s3_y_test = test_data_prep(torch.Tensor(np.asarray(s_test[3]))), torch.Tensor(np.asarray(s_y_test[3]))\n",
    "print('\\nsubject 4')\n",
    "s4_test, s4_y_test = test_data_prep(torch.Tensor(np.asarray(s_test[4]))), torch.Tensor(np.asarray(s_y_test[4]))\n",
    "print('\\nsubject 5')\n",
    "s5_test, s5_y_test = test_data_prep(torch.Tensor(np.asarray(s_test[5]))), torch.Tensor(np.asarray(s_y_test[5]))\n",
    "print('\\nsubject 6')\n",
    "s6_test, s6_y_test = test_data_prep(torch.Tensor(np.asarray(s_test[6]))), torch.Tensor(np.asarray(s_y_test[6]))\n",
    "print('\\nsubject 7')\n",
    "s7_test, s7_y_test = test_data_prep(torch.Tensor(np.asarray(s_test[7]))), torch.Tensor(np.asarray(s_y_test[7]))\n",
    "print('\\nsubject 8')\n",
    "s8_test, s8_y_test = test_data_prep(torch.Tensor(np.asarray(s_test[8]))), torch.Tensor(np.asarray(s_y_test[8]))\n",
    "\n",
    "#############################\n",
    "#Generate Subject DataLoader#\n",
    "#############################\n",
    "\n",
    "s0_train_loader = DataLoader(TensorDataset(s0_train, s0_y_train), batch_size=batch_size, shuffle=True)\n",
    "s0_val_loader = DataLoader(TensorDataset(s0_valid, s0_y_valid), batch_size=batch_size, shuffle=True)\n",
    "s0_test_loader = DataLoader(TensorDataset(s0_test, s0_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "s1_train_loader = DataLoader(TensorDataset(s1_train, s1_y_train), batch_size=batch_size, shuffle=True)\n",
    "s1_val_loader = DataLoader(TensorDataset(s1_valid, s1_y_valid), batch_size=batch_size, shuffle=True)\n",
    "s1_test_loader = DataLoader(TensorDataset(s1_test, s1_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "s2_train_loader = DataLoader(TensorDataset(s2_train, s2_y_train), batch_size=batch_size, shuffle=True)\n",
    "s2_val_loader = DataLoader(TensorDataset(s2_valid, s2_y_valid), batch_size=batch_size, shuffle=True)\n",
    "s2_test_loader = DataLoader(TensorDataset(s2_test, s2_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "s3_train_loader = DataLoader(TensorDataset(s3_train, s3_y_train), batch_size=batch_size, shuffle=True)\n",
    "s3_val_loader = DataLoader(TensorDataset(s3_valid, s3_y_valid), batch_size=batch_size, shuffle=True)\n",
    "s3_test_loader = DataLoader(TensorDataset(s3_test, s3_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "s4_train_loader = DataLoader(TensorDataset(s4_train, s4_y_train), batch_size=batch_size, shuffle=True)\n",
    "s4_val_loader = DataLoader(TensorDataset(s4_valid, s4_y_valid), batch_size=batch_size, shuffle=True)\n",
    "s4_test_loader = DataLoader(TensorDataset(s4_test, s4_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "s5_train_loader = DataLoader(TensorDataset(s5_train, s5_y_train), batch_size=batch_size, shuffle=True)\n",
    "s5_val_loader = DataLoader(TensorDataset(s5_valid, s5_y_valid), batch_size=batch_size, shuffle=True)\n",
    "s5_test_loader = DataLoader(TensorDataset(s5_test, s5_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "s6_train_loader = DataLoader(TensorDataset(s6_train, s6_y_train), batch_size=batch_size, shuffle=True)\n",
    "s6_val_loader = DataLoader(TensorDataset(s6_valid, s6_y_valid), batch_size=batch_size, shuffle=True)\n",
    "s6_test_loader = DataLoader(TensorDataset(s6_test, s6_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "s7_train_loader = DataLoader(TensorDataset(s7_train, s7_y_train), batch_size=batch_size, shuffle=True)\n",
    "s7_val_loader = DataLoader(TensorDataset(s7_valid, s7_y_valid), batch_size=batch_size, shuffle=True)\n",
    "s7_test_loader = DataLoader(TensorDataset(s7_test, s7_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "s8_train_loader = DataLoader(TensorDataset(s8_train, s8_y_train), batch_size=batch_size, shuffle=True)\n",
    "s8_val_loader = DataLoader(TensorDataset(s8_valid, s8_y_valid), batch_size=batch_size, shuffle=True)\n",
    "s8_test_loader = DataLoader(TensorDataset(s8_test, s8_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "subject_train_loader = [s0_train_loader, s1_train_loader, s2_train_loader, s3_train_loader, s4_train_loader, s5_train_loader, s6_train_loader, s7_train_loader,  s8_train_loader]\n",
    "subject_val_loader = [s0_val_loader, s1_val_loader, s2_val_loader, s3_val_loader, s4_val_loader, s5_val_loader, s6_val_loader, s7_val_loader, s8_val_loader]\n",
    "subject_test_loader = [s0_test_loader, s1_test_loader, s2_test_loader, s3_test_loader, s4_test_loader, s5_test_loader, s6_test_loader, s7_test_loader, s8_test_loader]\n",
    "\n",
    "print('Prepping Training Data')\n",
    "X_train, y_train = data_prep(X_train, y_train, subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nPrepping Validation Data')\n",
    "X_valid, y_valid = data_prep(X_valid, y_valid, subsample, average, noise, channel_dropout, time_reverse)\n",
    "print('\\nPrepping Test Data')\n",
    "X_test = test_data_prep(X_test)\n",
    "print('\\nFINISHED PREP\\n')\n",
    "\n",
    "print('Final shape of training set:', X_train.shape)\n",
    "print('Final shape of validation set:', X_valid.shape)\n",
    "print('Final shape of test set:', X_test.shape)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'TRAIN_DATA {len(train_loader)}\\nVALID_DATA {len(val_loader)}\\nTEST_DATA {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ebe27-42e8-48d9-bdf1-659aa6c1c5fa",
   "metadata": {},
   "source": [
    "# **GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf74b3-fd0c-43fb-aa37-d86e93c04ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "import nndl.models.GRU as gru\n",
    "gru_model = gru.GRU(hidden_size=128,\n",
    "          num_layers=3,\n",
    "          num_classes=4,\n",
    "          in_channels=22).to(device)\n",
    "\n",
    "torchinfo.summary(gru_model, input_size=(batch_size, 22, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70d547-87c2-461d-936c-53e44bcd8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.7\n",
    "kernel1 = 10\n",
    "kernel2 = (1, 22)\n",
    "kernel3 = 7\n",
    "kernel4 = 4\n",
    "pool_kernel = 3\n",
    "depth = 32\n",
    "cnn_model = cnn.CNN(num_classes=4,\n",
    "                dropout=dropout,\n",
    "                kernel1=kernel1,\n",
    "                kernel2=kernel2),\n",
    "                kernel3=kernel3,\n",
    "                kernel4=kernel4,\n",
    "                pool_kernel=pool_kernel,\n",
    "                depth=depth).to(device)\n",
    "\n",
    "\n",
    "torchinfo.summary(cnn_model, input_size=(batch_size, 22, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0828210-ea6e-4ebf-86e4-7aa105474199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb3bba-c33d-4217-97b7-ec6fd4aeb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_accuracies, val_accuracies = utils.train_model(model=model,\n",
    "                                                     criterion=criterion,\n",
    "                                                     optimizer=optimizer,\n",
    "                                                     scheduler=scheduler,\n",
    "                                                     train_loader=train_loader,\n",
    "                                                     val_loader=val_loader,\n",
    "                                                     num_epochs=20,\n",
    "                                                     learning=False,\n",
    "                                                     device=device,\n",
    "                                                     trial=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b878385-24b8-4681-925c-d30cbe2a8bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_epochs = 20\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (linux-venv)",
   "language": "python",
   "name": "linux-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
